{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanton99/QPU_NN_Project/blob/main/final_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quaternion Product Units for Deep Learning on 3D Rotation Groups\n",
        "\n",
        "Student names : Anton Volkov (Matricola) & Keerthana Kumaresan (1998292)\n",
        "\n",
        "The paper chosen is about using quaternion, quaternion algebra and the law of 3D rotation groups instead of Euclidean methods to create a mechanism to enhance the robustness of deep learning models to rotation. The proposed Quaternion Prouduct Unit represent the data as 3D pure quaternions (with the scalar being zero) and merges them through weighted chain of Hamilton products.\n",
        "\n",
        "$$\n",
        "q= s + ix + jy +kz ⊂ \\mathbb{H}\n",
        "$$\n",
        "\n",
        "where we have 1D real part and 3D imaginary part\n",
        "\n",
        " Such is used so we would be able to derive the \"Rotation Invariant\" and \"Rotation Equivarient\" features.\n",
        "\n",
        "$$\n",
        "\\mathbf{Rotation-invariance}: \\space f(R(x)) = f(x), \\forall x \\in X \\\\\n",
        "\\mathbf{Rotation-eqivariance}: \\space f(R(x)) = R(f(x)), \\forall x \\in X \\\\\n",
        "$$\n",
        "\n",
        "$x = [x_{1}, ...,x_{N}] \\in X$ contains $N$ rotations. The above feature support and improve the efficency in theory. The suggested QPU is implemented in a quaternion neural network and is made compatible with existing  deep learning models."
      ],
      "metadata": {
        "id": "qHQsrfDC0MUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QPU\n",
        "\n",
        "Each quaternion is represented as a unit quaternion, where the imaginary part gives the direction of its rotaion axis and the real part corresponds to the cosine of its rotation angle.\n",
        "\n",
        "$q_{out} = [s_{out},v_{out}] \\in \\mathbb{H}$ where the outputs gives us the rotation invariant and eqivariant part.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=15XPVmukWbq1-_dwH_UeokPYvoAsY_9e5' width = 70%/>\n",
        "</figure>\n",
        "\n",
        "We represent the quaternion removing the imaginary parts.\n",
        "\n",
        "\\begin{equation}\n",
        "q = [s, \\mathbf{v}] = [s, (x,y,z)]\n",
        "\\end{equation}\n",
        "\n",
        "The Hamilton product between two quaternions $q_{1} = [s_{1}, \\mathbf{v_{1}}]$ and $q_{2} = [s_{2}, \\mathbf{v_{2}}]$ is defined as\n",
        "\n",
        "\\begin{equation}\n",
        "q_{1} ⊗ q_{2} = [s_{1}s_{2} - (\\mathbf{v_{1}} \\cdot \\mathbf{v_{2}}), \\mathbf{v_{q}} \\times \\mathbf{v_{2}} + s_{1}\\mathbf{v_{2}} +  s_{2}\\mathbf{v_{1}}]\n",
        "\\end{equation}\n",
        "\n",
        "we must note that the hamilton product is not commutative, the the order of rotations matter when implementing it. If we rotate a 3D vector $\\mathbf{v_{1}}$ to another 3D vector $\\mathbf{v_{2}}$ and the rotation is with the axis $\\mathbf{u}$ and angle $\\theta$ then we define a unit quaternion as\n",
        "\n",
        "\\begin{equation}\n",
        "q = [s, \\mathbf{v}] = [cos\\left(\\frac{\\theta}{2}\\right),sin\\left(\\frac{\\theta}{2}\\right) \\mathbf{u}]\n",
        "\\end{equation}\n",
        "\n",
        "where $\\lVert \\mathbf{u} \\rVert_{2} = 1$ and $s^2 + \\lVert \\mathbf{v} \\rVert_{2}^2 = cos^2\\left(\\frac{\\theta}{2}\\right),sin^2\\left(\\frac{\\theta}{2}\\right) =1$. The vectors $\\mathbf{v_{1}}$ and $\\mathbf{v_{2}}$ as pure quaternions $[0, \\mathbf{v_{1}}]$ and $[0, \\mathbf{v_{2}}]$\n",
        "\n",
        "\\begin{equation}\n",
        "q = [0, \\mathbf{v_{2}}] = q \\otimes [0, \\mathbf{v_{1}}] \\otimes q^*\n",
        "\\end{equation}\n",
        "\n",
        "The $q^* = [0, \\mathbf{-v}]$ is the conjugate of $q$.\n",
        "\n",
        "The combination of rotation matrices can also be denoted as such with Hamilton products.\n",
        "\n",
        "\\begin{equation}\n",
        "(q_{2} \\otimes q_{1})  \\otimes [0, \\mathbf{v_{1}}] \\otimes (q^*_1 \\otimes q^*_2)\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "G0rOwcg-1ZO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODELS\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1M6_beQ3FZhl5GOWJRUZuQpNo_e_uHDp9' width = 70%/>\n",
        "</figure>\n",
        "\n",
        "We are replacing the neurons in a standard deep learning model with weighted chain of Hamilton product. Usually the neurons are represented as a weighted summation unit where there are learnable parameters, inputs and a non linear activation function.\n",
        "\n",
        "A problem with the quaternion proposed above is that it does not guarantee the closer of $\\mathbb{SO}(3)$ so we make them weighted and it is defined as such, for $N$ unit quaternions, the product is\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\bigotimes^N_{i=1} q^{w_{i}}_i = q_1^{w_{1}} \\otimes q_2^{w_{2}} \\otimes ... \\otimes q_N^{w_{N}}\n",
        "\\end{equation}\n",
        "\n",
        "The power of the quaternion is defined in the paper but for simplification has not been included here but this is being implemented in the code. One important note is that the power of the quaternion only scales the rotation angle but does not change the rotation axis.\n",
        "\n",
        "\\begin{equation}\n",
        "QPU(\\{q_i\\}^N_{i=1}; \\{ w_i\\}_{i=1}^N,b) = \\bigotimes^N_{i=1} qpow(q_i;w_i, b)\n",
        "\\end{equation}\n",
        "\n",
        "where for $q_i =[s_i,\\mathbf{v_i}]$  and  \\\\\n",
        "$qpow(q_i;w_i, b) = [cos(w_i(arcos(s_i) + b)), \\frac{\\mathbf{v_i}}{\\lVert \\mathbf{v_i}\\rVert_2} sin (w_i(arcos(s_i) +b))]$ represents the weights $w_i$ and a bias $b$.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1lmSkoOiFs_epW09DQaGHxVPEJJ2C5b78' width = 70%/>\n",
        "</figure>\n",
        "\n",
        "The multiple QPUs recieving the input  quaternions for a FC-layer. Since the QPU itself is nonlinear in nature, we have no activation function added. The output of a QPU is a quaternion, so we connect multiple QPU-based FC layers to form a Quaternion Multi-Layer Perceptron (QMLP) model.\n",
        "\n",
        "Three models are shown in the picture above, where one has already been explain and the QMLP-RInv is a variant of QMLP and RMLP is the standard real-valued MLP. All the models consists of three layer.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2QlgCkmL13Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASET GENERATION\n",
        "\n",
        "#### CUBE EDGE\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-uN02rj6LpVbLVWzMlnPddkOEztP7Mas' width = 70%/>\n",
        "</figure>\n",
        "\n",
        "A dataset is generated to test the robustness of our QPU based models. The dataset consits of partial skeletons of an cube. The number of edges is selected first and the in the exam we have a used 4, 5, 6 and 7 number of edges. and as the edges increase the data samples increase. To be concise on how the datset is derived, an edge is considered and depending on the number of edges chosen, one of the adjacent edge of the previous edge is chosen to for a skeleton. Samples can be seen in the above picture and a set of sample are plotted at the end of the code.\n"
      ],
      "metadata": {
        "id": "awnonblQ0WpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAIN AND TEST\n",
        "\n",
        "#### Backpropagation of QPU\n",
        "\n",
        "Computing the gradient of the weighting function are easy but the gradient of the Hamilton product chain is not simple. The diffrential of the quaternion is calculated and this is explained more detailed in the paper but has been excluded here.\n",
        "\n",
        "The computation of  the differential of loss $L$ for the quaternion is done (computed with autograd in Pytorch)."
      ],
      "metadata": {
        "id": "9DkdWkpo18s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from train import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "from models import *\n",
        "from cubeEdgeData import CubeEdge"
      ],
      "metadata": {
        "id": "UjQASJlgfzI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "        \"cuda\" if torch.cuda.is_available()\n",
        "        else \"mps\" if torch.backends.mps.is_available()\n",
        "        else \"cpu\"\n",
        "    )"
      ],
      "metadata": {
        "id": "q5Fz91Xyjq9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train QMLP_RINV\n",
        "start_time = time.time()\n",
        "training_data = CubeEdge(train=True, num_edges=7, use_quaternion=True,num_samples=500)\n",
        "test_data = CubeEdge(train=False,num_edges=7,use_quaternion=True,num_samples=50)\n",
        "net = QMLP_RInv(num_data=7,num_cls=training_data.num_shapes)\n",
        "print(\"Network being trained is QMLP_RInv\")\n",
        "train(model=net,data=training_data,epochs=100,lr=0.01,batch_size=training_data.num_shapes)\n",
        "print(\"Training is completed\")"
      ],
      "metadata": {
        "id": "qxNFztqujjEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model=net,data=test_data)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "qhVv6-gxjx3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train QMLP\n",
        "start_time = time.time()\n",
        "training_data = CubeEdge(train=True, num_edges=7, use_quaternion=True,num_samples=500)\n",
        "test_data = CubeEdge(train=False,num_edges=7,use_quaternion=True,num_samples=50)\n",
        "net = QMLP(num_data=7,num_cls=training_data.num_shapes)\n",
        "print(\"Network being trained is QMLP\")\n",
        "train(model=net,data=training_data,epochs=100,lr=0.01,batch_size=training_data.num_shapes)\n",
        "print(\"Training is completed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvqHiXlMaez-",
        "outputId": "a8c5ad5a-57b1-41bd-e081-a930eddd94b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network being trained is QMLP\n",
            "epoch: 10\n",
            "tensor(0.2976, grad_fn=<NllLossBackward0>)\n",
            "epoch: 20\n",
            "tensor(0.2357, grad_fn=<NllLossBackward0>)\n",
            "epoch: 30\n",
            "tensor(0.4005, grad_fn=<NllLossBackward0>)\n",
            "epoch: 40\n",
            "tensor(0.4592, grad_fn=<NllLossBackward0>)\n",
            "epoch: 50\n",
            "tensor(0.1147, grad_fn=<NllLossBackward0>)\n",
            "epoch: 60\n",
            "tensor(0.5859, grad_fn=<NllLossBackward0>)\n",
            "epoch: 70\n",
            "tensor(0.3401, grad_fn=<NllLossBackward0>)\n",
            "epoch: 80\n",
            "tensor(0.3328, grad_fn=<NllLossBackward0>)\n",
            "epoch: 90\n",
            "tensor(0.1144, grad_fn=<NllLossBackward0>)\n",
            "epoch: 100\n",
            "tensor(0.3935, grad_fn=<NllLossBackward0>)\n",
            "Training is completed\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.325249 \n",
            "\n",
            "--- 102.6377477645874 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model=net,data=test_data)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "W5honnKYj2nN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}